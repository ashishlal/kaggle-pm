{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "import xgboost as xgb\n",
    "import feather\n",
    "from sklearn.base import BaseEstimator as be\n",
    "from sklearn.base import TransformerMixin as tm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any results you write to the current directory are saved as output.\n",
    "submission = pd.read_csv('../data/stage2_sample_submission.csv')\n",
    "stage1_test = pd.read_csv('../data/test_variants')\n",
    "stage2_test = pd.read_csv('../data/stage2_test_variants.csv')\n",
    "stage1_solution = pd.read_csv('../data/stage1_solution_filtered.csv')\n",
    "\n",
    "stage1_solution = stage1_solution.merge(stage1_test, how = 'left', on = 'ID')\n",
    "\n",
    "stage2_hack = stage2_test.merge(\n",
    "        stage1_solution.drop('ID', axis = 1), \n",
    "        how = 'left', \n",
    "        on = ['Gene', 'Variation'])\\\n",
    "    .drop(['Gene', 'Variation'], axis = 1)\\\n",
    "    .fillna(1)\n",
    "#     .to_csv('../cache/stage2_submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_hack_ids = stage2_hack.ID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_hack = stage2_hack.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_hack['Class'] = stage2_hack.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_hack['Class'] = stage2_hack['Class'].map(lambda x: int(x[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_hack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_hack = stage2_hack.drop(['class1','class2','class3','class4','class5','class6','class7','class8','class9'],\n",
    "                               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_hack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_hack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = feather.read_dataframe('../cache/train_stage2_fe.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = df_train.ID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hack = stage2_hack['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((y, y_hack), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = feather.read_dataframe('../cache/test_stage2_fe.feather')\n",
    "pid = df_test.ID\n",
    "df_test = df_test.drop('ID', axis=1)\n",
    "df_test['Class'] = y_hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat((df_train, df_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = feather.read_dataframe('../cache/test_stage2_fe.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=True):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    my_stopwords = [\n",
    "        \"fig\", \"figure\", \"et\", \"al\", \"table\",\n",
    "        \"data\", \"analysis\", \"analyze\", \"study\",\n",
    "        \"method\", \"result\", \"conclusion\", \"author\",\n",
    "        \"find\", \"found\", \"show\", \"perform\",\n",
    "        \"demonstrate\", \"evaluate\", \"discuss\"\n",
    "    ]\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = [w for w in text if not w in my_stopwords]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cust_regression_vals(be, tm):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        x = x.drop(['Gene', 'Variation', 'ID','Text', 'GeneVar'],axis=1).values\n",
    "        return x\n",
    "\n",
    "class cust_txt_col(be, tm):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        return x[self.key].apply(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fp = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        n_jobs = -1,\n",
    "        transformer_list = [\n",
    "            ('standard', cust_regression_vals()),\n",
    "            ('pi1', Pipeline([('Gene', cust_txt_col('Gene')), \n",
    "                                       ('count_Gene', CountVectorizer(analyzer=u'char',ngram_range=(1, 3))), \n",
    "                                       ('tsvd1', TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n",
    "            ('pi2', Pipeline([('Variation', cust_txt_col('Variation')), \n",
    "                                       ('count_Variation', CountVectorizer(analyzer=u'char',ngram_range=(1, 3))), \n",
    "                                       ('tsvd2', TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n",
    "            ('pi3', Pipeline([('GeneVar', cust_txt_col('GeneVar')), \n",
    "                                       ('count_GeneVar', CountVectorizer(analyzer=u'char', ngram_range=(1, 3))), \n",
    "                                       ('tsvd2', TruncatedSVD(n_components=20, n_iter=25, \n",
    "                                                                            random_state=12))])),\n",
    "            ('pi4',Pipeline([('Text', cust_txt_col('Text')), \n",
    "                                       ('hv', HashingVectorizer(decode_error='ignore', \n",
    "                                                                                        n_features=2 ** 16,\n",
    "                                                                                        non_negative=True, \n",
    "                                                                                        ngram_range=(1, 5))),\n",
    "                                       ('tfidf_Text', TfidfTransformer()), \n",
    "                                       ('tsvd3', TruncatedSVD(n_components=300, n_iter=25, \n",
    "                                                                            random_state=12))]))\n",
    "\n",
    "        \n",
    "        ])\n",
    "    )])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Text'] = [text_to_wordlist(w) for w in df_train['Text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Text'] = [text_to_wordlist(w) for w in df_test['Text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ID'] = [i for i in range(df_train.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = df_train\n",
    "te = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tr['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(te.columns) - set(tr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.drop('ID', axis=1)\n",
    "# df_test = df_test.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = fp.fit_transform(df_train)\n",
    "print (df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = fp.fit_transform(df_test)\n",
    "print (df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y - 1 #fix for zero bound array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "denom = 0\n",
    "fold = 10 \n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'eta': 0.02,\n",
    "        'max_depth': 5,\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 9,\n",
    "        'seed': i,\n",
    "        'silent': True\n",
    "    }\n",
    "    x1, x2, y1, y2 = train_test_split(df_train, y, test_size=0.2, random_state=i)\n",
    "    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n",
    "    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=100)\n",
    "    score1 = log_loss(y2, model.predict(xgb.DMatrix(x2), \n",
    "                                                ntree_limit=model.best_ntree_limit), labels = list(range(9)))\n",
    "    print(score1)\n",
    "    #if score < 0.9:\n",
    "    if denom != 0:\n",
    "        pred = model.predict(xgb.DMatrix(df_test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = model.predict(xgb.DMatrix(df_test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds = pred.copy()\n",
    "    denom += 1\n",
    "    submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
    "    submission['ID'] = pid\n",
    "    submission.to_csv('../submissions/submission2_3_xgb_fold_'  + str(i) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds/denom, columns=['class'+str(c+1) for c in range(9)])\n",
    "submission['ID'] = pid\n",
    "submission.to_csv('../submissions/submission_all_2_3_xgb.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.read_csv('../submissions/submission_all_2_3_xgb.csv',index_col=False)\n",
    "df_preds = df_preds.drop('ID', axis=1)\n",
    "df_preds['class'] = df_preds.idxmax(axis=1)\n",
    "df_preds['class'] = df_preds['class'].str[-1].astype(int)\n",
    "df_preds['class'] = df_preds['class'] -1\n",
    "df_preds = df_preds.drop(['class1','class2','class3','class4','class5','class6',\n",
    "                          'class7','class8','class9'],axis=1)\n",
    "y_pseudo = df_preds['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate((y, y_pseudo), axis=0)\n",
    "X = np.concatenate((df_train, df_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom = 0\n",
    "fold = 5\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'eta': 0.02,\n",
    "        'max_depth': 5,\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 9,\n",
    "        'seed': i,\n",
    "        'silent': True,\n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "    x1, x2, y1, y2 = train_test_split(X, Y, test_size=0.2, random_state=i)\n",
    "    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n",
    "    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=100)\n",
    "    score1 = log_loss(y2, model.predict(xgb.DMatrix(x2), \n",
    "                                                ntree_limit=model.best_ntree_limit), labels = list(range(9)))\n",
    "    print(score1)\n",
    "    #if score < 0.9:\n",
    "    if denom != 0:\n",
    "        pred = model.predict(xgb.DMatrix(df_test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = model.predict(xgb.DMatrix(df_test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds = pred.copy()\n",
    "    denom += 1\n",
    "    submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
    "    submission['ID'] = pid\n",
    "    submission.to_csv('../submissions/submission_2_2_3_xgb_fold_'  + str(i) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds/denom, columns=['class'+str(c+1) for c in range(9)])\n",
    "submission['ID'] = pid\n",
    "submission.to_csv('../submissions/submission_all_2_2_3_xgb.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
